 <h3 id="conjectures">CONJECTURES</h3>
 <ul>
   <li> My favorite conjectures (at least these days) lie in the realm found at the intersection of Boolean Function Analysis, Information Theory, Isoperimetric inequalities, Hardness of Approximation and Communication Complexity.
 <ul>
   <li><strong>Most Informative Boolean Function Conjecture</strong> : A deceptively simple conjecture proposed by <a href="https://people.eecs.berkeley.edu/~courtade/pdfs/CourtadeKumar_TransITAug2014.pdf">Courtade et al</a>. It essentially hypothesizes (a very natural intuition) that the information provided by the most informative bit in a bit string (drawn uniformly at random) sent through a Binary Symmetric Channel (with noise 'p') is no more than the capacity of the channel. Despite its appearances, it is hard to prove the conjecture and it appears that reasoning about the influence of the function and thinking isoperimetrically may aid in leading towards a proof; Intuitively, the functions that should provide the maximum information are the dictator functions as they faithfully relay a coordinate, and it can be guessed up to the capacity of the channel. The function has been show to be true in the high-noise regime by <a href="https://arxiv.org/pdf/1510.08656.pdf">Samorodnitsky (2015)</a>.</li>
   <li><strong>Fourier Entropy-Influence Conjecture</strong>: This conjecture was first posited by Friedgut-Kalai in 1996 while investigating monotonic predicates over random graphs and attempting to understand what distribution over the fourier coefficients of boolean functions causes sharp-threshold phenomenon for these predicates. The intuition is that having a spectral distribution that maximizes the entropy causes this phenomenon. The conjecture as it stands has only been shown
     to be true for <a href="https://www.cs.cmu.edu/~jswright/papers/fei.pdf">symmetric boolean functions</a> and those that have <a href="https://arxiv.org/pdf/1806.03646.pdf">large entropy and small influence</a>. </li>
   <li><strong>Log-Rank Conjecture</strong>: Probably the most well known conjecture of all, the Log-Rank conjecture is a long standing conjecture proposed by <a href="http://delivery.acm.org/10.1145/810000/804414/p209-yao.pdf?ip=128.103.224.4&id=804414&acc=ACTIVE%20SERVICE&key=AA86BE8B6928DDC7%2EC82FBC3DCC335AD2%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1566619408_830e62b6051cb33d77bbc1bdbee5b7a4">Yao (1979)</a> and states that the deterministic communication complexity (the number of bits
     of the input x given to Alice and y given to Bob that need to be communicated to compute f(x,y)) to compute any joint function is poly-logarithmic in the rank of the communication matrix. The
     conjecture is quite intuitive, and playing around with standard functions such as EQ(x, y) [which is full-rank in the communication matrix] or IP(x,y) helps build the intuition for the conjecture. The tightest bound on CC(f) is by <a href="https://arxiv.org/abs/1306.1877">Lovett (2014)</a> using the Discrepancy method.</li>
 </ul></li>
 </ul>
